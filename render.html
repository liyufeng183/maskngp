<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Renderer</title>
    <meta charset="UTF-8" />

    <meta
      name="viewport"
      content="width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0"
    />

    <style>
      body,
      div {
        margin: 0;
        padding: 0;
      }
      .renderer {
        border: dashed #000000;
      }
    </style>
  </head>

  <body>
    <div
      id="container"
      style="display: flex; justify-content: center; align-items: center"
    ></div>
    <div id="progressBar" style="text-align: center"></div>
  </body>

  <script type="module">
    import * as THREE from "https://unpkg.com/three?module";
    import WebGL from "https://unpkg.com/three/examples/jsm/capabilities/WebGL.js?module";
    import { OBJLoader } from "https://unpkg.com/three/examples/jsm/loaders/OBJLoader.js?module";
    import { OrbitControls } from "https://unpkg.com/three/examples/jsm/controls/OrbitControls.js?module";
    import Stats from "https://unpkg.com/three/examples/jsm/libs/stats.module.js?module";
    import { GUI } from "https://unpkg.com/three/examples/jsm/libs/lil-gui.module.min.js?module";

    // const BASE =
    //   "https://huggingface.co/ashawkey/nerf2mesh/resolve/main/scenes/"; // remote
    //   "https://huggingface.co/ashawkey/nerf2mesh"; // remote
    const BASE = "../maskngp/scenes/"; // local

    // shaders
    const RenderVertShader = `
in vec3 position;  // 输入的顶点位置
in vec2 uv;        // 输入的UV坐标
in vec3 normal;    // 输入的顶点法线

out vec2 vUv;      // 输出的UV坐标,将被传递到片元着色器
out vec3 Normal;   // 输出的顶点法线，将被传递到片元着色器
// out vec3 rayDirection; // 输出的射线方向，将被传递到片元着色器
out vec3 positionF;
out vec3 cameraPos;
out mat4 modelPos;
out mat3 nomalM;
uniform mat4 modelViewMatrix; // 模型视图矩阵（将模型空间的顶点位置转换到视图空间）
uniform mat4 projectionMatrix;  // 投影矩阵（将视图空间的顶点位置转换到裁剪空间）
uniform mat4 modelMatrix;        // 模型矩阵（将模型空间的顶点位置转换到世界空间）
uniform vec3 cameraPosition; // 摄像机的位置
uniform mat3 normalMatrix;       // 模型视图矩阵的逆转置，用于正确转换法线

void main() {
    vUv = uv; // 将输入的UV坐标传递到片元着色器
    // Normal =  normalMatrix*normal;//将模型空间的法线转换到视图空间
    Normal=normal;
    gl_Position = projectionMatrix * modelViewMatrix * vec4( position, 1.0 ); // 计算裁剪空间的顶点位置
    // rayDirection = (  cameraPosition-(modelMatrix * vec4( position, 1.0 )).rgb ); // 计算从摄像机位置到世界空间的顶点位置的射线方向
    positionF=position;
    cameraPos= cameraPosition;
    modelPos=modelMatrix;
    nomalM=normalMatrix;

}
`;
    //需要注意的是，这个着色器假设所有输入的顶点属性（如位置、UV坐标和法线）都已经在模型空间中。这意味着如果模型进行了变换
    //（如旋转、缩放或平移），这些变换已经应用到了这些属性上。如果模型的变换存储在模型矩阵中，
    //并且模型的顶点属性是相对于模型的本地坐标系的，那么你可能需要在计算Normal和rayDirection之前应用模型矩阵。
    const RenderFragShader_template = `
precision highp float;  // 设置浮点数的精度为高精度

layout(location = 0) out vec4 pc_FragColor;  // 输出的颜色值

in vec2 vUv;  // 从顶点着色器接收的UV坐标
in vec3 Normal;  // 从顶点着色器接收的法线
in mat4 modelPos;
in  mat3 nomalM;
// in vec3 rayDirection;  // 从顶点着色器接收的射线方向
in vec3 positionF;
in vec3 cameraPos;
uniform int mode;  // 渲染模式
uniform highp sampler2D Diffuse;  // diff纹理
uniform highp sampler2D hash0;//specular 特征
uniform highp sampler2D hash1;//specular 特征
// uniform highp sampler2D hash2;//specular 特征
// uniform highp sampler2D hash3;//specular 特征
// uniform highp sampler2D hash4;//specular 特征

uniform highp sampler2D weightsZero;  // 第一层神经网络的权重,已经存储为sampler2D的纹理采样器格式
uniform highp sampler2D weightsOne;  // 第二层神经网络的权重
uniform highp sampler2D BiasZero;  // 第一层神经网络的偏移
uniform highp sampler2D BiasOne;  // 第二层神经网络的偏移
#define PI 3.14159265358979323846

float[39] frencode(vec3 dir, int degree) {//返回编码完成后的dir数组(39)
    float[39] result;//3*6*2+3
    result[0]=dir.x;
    result[1]=dir.y;
    result[2]=dir.z;
    
    for (int i = 0; i <6; ++i) {
    float valx = pow(2.0, float(i))  * (dir.x);
    float valy = pow(2.0, float(i))  * (dir.y);
    float valz = pow(2.0, float(i))  * (dir.z);

        //sin,cos交替
        // result[i*degree+3]=sin(valx);
        // result[i*degree+3+1]=cos(valx);
        // result[i*degree+3+2]=sin(valy);
        // result[i*degree+3+3]=cos(valy);
        // result[i*degree+3+4]=sin(valz);
        // result[i*degree+3+5]=cos(valz);
      //先sin,再cos
        result[i*degree+3]=sin(valx);
        result[i*degree+3+1]=sin(valy);
        result[i*degree+3+2]=sin(valz);
        result[i*degree+3+3]=cos(valx);
        result[i*degree+3+4]=cos(valy);
        result[i*degree+3+5]=cos(valz);
      //交错进行
        // result[i*degree+3]=sin(valx);
        // result[i*degree+3+1]=cos(valy);
        // result[i*degree+3+2]=sin(valz);
        // result[i*degree+3+3]=cos(valx);
        // result[i*degree+3+4]=sin(valy);
        // result[i*degree+3+5]=cos(valz);

        // result[i*degree+3]=cos(valx);
        // result[i*degree+3+1]=sin(valy);
        // result[i*degree+3+2]=cos(valz);
        // result[i*degree+3+3]=sin(valx);
        // result[i*degree+3+4]=cos(valy);
        // result[i*degree+3+5]=sin(valz);
    }
    
    return result;
}





// evaluateNetwork(hash_feature, normalize(rayDirection),normal)
vec3 evaluateNetwork(float[5] hash, float[39] dir,vec3 normal) {

    // NUM_CHANNELS_ZERO (input_dim) is hard-coded as 6->47
    // NUM_CHANNELS_ONE (hidden_dim) can vary, but should be divisible by 4  32->64
    // NUM_CHANNELS_TWO (output_dim) is hard-coded as 3
    mat4 w;
    float[48] v;
    vec4 bias;
    
    for(int i=0;i<48;i++){
      if(i <39){
      v[i]=dir[i];
      // v.data[i]=0.0;
      }
      else if(i<42){
        v[i]=normal.x;
        // v.data[i]=0.0;
        i=i+1;
        v[i]=normal.y;
        // v.data[i]=0.0;
        i=i+1;
        v[i]=normal.z;
        // v.data[i]=0.0;


      }
      else if(i<47){
        v[i]=hash[i-42];
        // v.data[i]=0.0;

      }
      else{
        v[i]=0.0;//填充0
      }
    }






    vec4 result_one[NUM_CHANNELS_ONE / 4];//64个中间层
    int offset=0;
    for (int j = 0; j < 48; j += 4) {
      
    vec4 tempv = vec4(
        v[j],
        v[j+1],
        v[j+2],
        v[j+3]
    );
    
    // texelFetch(weightsZero, ivec2(0, NUM_CHANNELS_ONE + i), 0),
    for (int i = 0; i < NUM_CHANNELS_ONE; i += 4) {
      
            w = mat4(
                texelFetch(weightsZero, ivec2(0,offset+ i), 0),
                texelFetch(weightsZero, ivec2(0, offset+i + 1), 0),
                texelFetch(weightsZero, ivec2(0,offset+ i + 2), 0),
                texelFetch(weightsZero, ivec2(0,offset+i + 3), 0)
            );
            // if(j==0){}
         
            if(j==0){
              bias=vec4(
              // texelFetch(BiasZero, ivec2(0,i/4), 0)
              texelFetch(BiasZero, ivec2(0,i), 0).r,
              texelFetch(BiasZero, ivec2(0,i+1), 0).r,
              texelFetch(BiasZero, ivec2(0,i+2), 0).r,
              texelFetch(BiasZero, ivec2(0,i+3), 0).r
            );
            result_one[i / 4] += tempv * w+bias;
            }
            else {
              result_one[i / 4] += tempv * w;
            }
            // result_one[i / 4] += tempv * w;
        }
        offset=offset+(1*NUM_CHANNELS_ONE);
    }

    vec3 result;

    for (int i = 0; i < NUM_CHANNELS_ONE / 4; i++) {
        vec4 tempv2 = max(result_one[i], 0.0); // relu
        // vec4 tempv2=result_one[i];
        w = mat4(
            texelFetch(weightsOne, ivec2(0, i * 3), 0),
            texelFetch(weightsOne, ivec2(0, i * 3 + 1), 0),
            texelFetch(weightsOne, ivec2(0, i * 3 + 2), 0),
            vec4(0.0,0.0,0.0,0.0) // padding
        );
        
       
        if(i==0){
          bias=vec4(
          // texelFetch(BiasOne, ivec2(0,0), 0)
              texelFetch(BiasOne, ivec2(0,0), 0).r,
              texelFetch(BiasOne, ivec2(0,1), 0).r,
              texelFetch(BiasOne, ivec2(0,2), 0).r,
              texelFetch(BiasOne, ivec2(0,3), 0).r
            );
        result += (tempv2 * w+bias).xyz;}
        else{
          result += (tempv2 * w).xyz;
        }
        // result += (tempv2 * w).xyz;
    }
    // vec3 test=vec3(0.35,0.35,0.36);//说明fragcolor的值为0-1之间,而现在返回的值其实就是0.5左右的固定错误值
    // result= -result;
    return 1.0 / (1.0 + exp(-result)); 
    

}

void main() {    
  vec3 rayDirection = ( cameraPos-(modelPos*vec4( positionF, 1.0 )).rgb ); 
  vec3 rayDirection2 = ( cameraPos-(vec4( positionF, 1.0 )).rgb ); 
  vec3 dis= rayDirection- rayDirection2;
  vec3 normal=Normal;
  // vec3 normal2=Normal;
  // vec3 disN=normal-normal2;
  // rayDirection = ((modelMatrix * vec4( positionF, 1.0 )).rgb - cameraPosition); 
    // if(mode ==2) {
        float[5] hash_feature;//获取32维的hash特征
        float[39] Dir=frencode((normalize(rayDirection)),6);//获取39维的归一化后fre编码的dir
        // FreArray Dir=frencode(((rayDirection)),6);//获取39维的dir
        // vec4 thash0=texture(hash0,vUv);//float类型
       
        vec4 thash0=texture(hash0,vUv);//float类型
        vec4 hash0=thash0;
        // thash0=(thash0/65535.0)*15.2-6.97;//返归一化
        // thash0=(thash0)*15.2-6.97;//返归一化
        
        thash0=(thash0)*16.92-7.69;//4096
        vec4 thash1=texture(hash1,vUv);
        vec4 hash1=thash1;
        // thash1=(thash1/65535.0)*15.2-6.97;
        // thash1=(thash1)*15.2-6.97;//返归一化
      
        thash1=(thash1)*16.92-7.69;//4096
        hash_feature[0]     = thash0.r;
        hash_feature[1]     = thash0.g;
        hash_feature[2]     = thash0.b;
        hash_feature[3]     = thash1.g;
        hash_feature[4]     = thash1.b;

        vec4 diffuse = texture( Diffuse, vUv );
        // vec3 normal=vec3(normalize(Normal).x,normalize(Normal).y,normalize(Normal).z);
        // if (mode == 2) { // 如果渲染模式为2(只渲染高光），则输出颜色为通过神经网络计算的高光颜色
            vec3 testn=vec3(0.0,0.0,0.0);
            float[5] testh;
            float[39] testr;
            //Dir应该是对的,Hash和normal随便初始化一个都一样,说明完全不对
            // pc_FragColor.rgb = evaluateNetwork(testh, testr,normalize(Normal));
            // pc_FragColor.rgb = evaluateNetwork(hash_feature, (Dir),normalize(Normal))/255.0;
            // vec3 tSpecular =clamp( evaluateNetwork(hash_feature, (Dir),normalize(Normal)),0.0f,1.0f);
            vec3 tSpecular =evaluateNetwork(hash_feature, (Dir),normalize(Normal));
            // vec3 tSpecular =evaluateNetwork(hash_feature, (testr),normalize(testn));
            // pc_FragColor.rgb = clamp((diffuse.rgb + evaluateNetwork(hash_feature, (Dir),normalize(Normal)))/2.0, 0.0f, 1.0f);
            pc_FragColor.rgb =((diffuse.rgb)+(tSpecular.rgb))/2.0;
          

        // } 
    // }
    pc_FragColor.a = 1.0;  // 设置颜色的透明度为1(不透明）
    // pc_FragColor.rgb = diffuse.rgb;
    // pc_FragColor = vec4(tSpecular.rgb,1.0);
    // pc_FragColor = vec4(0.0,0.0,0.0, 1.0);
    // pc_FragColor =vec4(Dir[36],Dir[37],Dir[38], 1.0);
    // pc_FragColor = vec4(positionF.xyz, 1.0);
    // pc_FragColor = vec4(disN.xyz, 1.0);
    // pc_FragColor = vec4(rayDirection.xyz, 1.0);
    // pc_FragColor = vec4(cameraPos, 1.0);
    // pc_FragColor = vec4(vUv.xy,1.0, 1.0);
    // pc_FragColor = vec4(normalize(normal).xyz, 1.0);
    // pc_FragColor = vec4(hash0.rgb,1.0);
    // pc_FragColor = vec4(hash1.rgb,1.0);
    // pc_FragColor = vec4(hash1,hash2,hash3,1.0);
    // pc_FragColor = vec4(hash2,hash3,hash4,1.0);
    // pc_FragColor = vec4(thash1,1.0,1.0,1.0);
    // pc_FragColor = vec4(thash2,1.0,1.0,1.0);
    // pc_FragColor = vec4(thash3,1.0,1.0,1.0);
    // pc_FragColor = vec4(thash4,1.0,1.0,1.0);
}
`;

    function createNetworkWeightTexture(network_weights) {
      let width = network_weights.length; //6->74,32->64
      let height = network_weights[0].length; //32->64,3->3

      console.log("宽度是", width);
      console.log("高度是", height);

      let weightsData = new Float32Array(width * height);
      for (let co = 0; co < height; co++) {
        for (let ci = 0; ci < width; ci++) {
          let index = co * width + ci; // column-major
          let weight = network_weights[ci][co];
          weightsData[index] = weight;
        }
      }

      let width_pad = width + (4 - (width % 4)); // make divisible by 4
      let weightsData_pad = new Float32Array(width_pad * height);
      for (let j = 0; j < width_pad; j += 4) {
        for (let i = 0; i < height; i++) {
          for (let c = 0; c < 4; c++) {
            if (c + j >= width) {
              weightsData_pad[j * height + i * 4 + c] = 0.0; // zero padding
            } else {
              weightsData_pad[j * height + i * 4 + c] =
                weightsData[j + i * width + c];
            }
          }
        }
      }

      let texture = new THREE.DataTexture(
        weightsData_pad,
        1,
        (width_pad * height) / 4,
        THREE.RGBAFormat,
        THREE.FloatType
      );
      texture.magFilter = THREE.NearestFilter;
      texture.minFilter = THREE.NearestFilter;
      texture.needsUpdate = true;
      // console.log("函数中的texture is: ",texture);
      return texture;
    }

    function createNetworkBias(network_bias) {
      let width = network_bias.length; //64,3
      // let height = network_weights[0].length; //32->64,3->3

      console.log("bias的长度是", width);
      // console.log('高度是',height)

      let weightsData = new Float32Array(width);

      for (let ci = 0; ci < width; ci++) {
        // let index = ci; // column-major
        // let weight = network_bias[ci];
        // weightsData[index] = weight;
        weightsData[ci] = network_bias[ci];
      }
      let width_pad;
      if (width % 4 != 0) {
        width_pad = width + (4 - (width % 4));
      } // make divisible by 4
      else {
        width_pad = width;
      }
      console.log("当前bisa填充后的长度", width_pad);
      let weightsData_pad = new Float32Array(width_pad);

      for (let i = 0; i < width_pad; i += 1) {
        if (i < width) {
          weightsData_pad[i] = weightsData[i];
        } else {
          weightsData_pad[i] = 0.0;
        }
      }
      console.log("当前的偏移未填充时如下", weightsData);
      console.log("当前的偏移填充如下", weightsData_pad);
      let texture = new THREE.DataTexture(
        weightsData_pad,
        1,
        width_pad,
        // THREE.RGBAFormat,
        THREE.RedFormat,
        THREE.FloatType
      );
      texture.magFilter = THREE.NearestFilter;
      texture.minFilter = THREE.NearestFilter;
      texture.needsUpdate = true;
      // console.log("函数中的texture is: ",texture);
      return texture;
    }

    function createViewDependenceFunctions(network_weights) {
      let channelsZero = network_weights["net.0.weight"].length; //47
      let channelsOne = network_weights["net.1.weight"].length; //64
      let channelsTwo = network_weights["net.1.weight"][0].length; //3
      // console.log('')
      console.log("[INFO] load MLP: ", channelsZero, channelsOne, channelsTwo);

      let RenderFragShader = RenderFragShader_template.replace(
        new RegExp("NUM_CHANNELS_ZERO", "g"),
        channelsZero
      );
      RenderFragShader = RenderFragShader.replace(
        new RegExp("NUM_CHANNELS_ONE", "g"),
        channelsOne
      );
      RenderFragShader = RenderFragShader.replace(
        new RegExp("NUM_CHANNELS_TWO", "g"),
        channelsTwo
      );

      return RenderFragShader;
    }

    let container,
      params,
      progressBar,
      progress,
      scene,
      camera,
      renderer,
      controls,
      stats,
      configs,
      sceneRef;

    // support changing scene name from url param
    // e.g. ?scene=lego&scene=chair
    //获取当前URL的值
    params = new URLSearchParams(new URL(window.location.href).searchParams);
    // let url = window.location.href;
    //记录当前页面的url
    // console.log(url);
    // document.getElementById("demo").innerHTML = url;

    const scene_names = params.getAll("scene");
    //传入场景的名称
    // const scene_names = "scene=lego&scene=chair&scene=bicycle";
    console.log(scene_names);
    // global config
    configs = {
      bg_color:
        params.get("bg_color") === null
          ? 0xffffff
          : parseInt(params.get("bg_color")), // default is white
      H: parseInt(params.get("H")) || Math.floor(0.95 * window.innerHeight),
      W: parseInt(params.get("W")) || Math.floor(0.99 * window.innerWidth),
      fovy: parseInt(params.get("fovy")) || 30,
      near: parseFloat(params.get("near")) || 0.01,
      far: parseFloat(params.get("far")) || 100,
      cameraState: params.get("cameraState"),
    };
    //记录config的值
    console.log(configs);
    function render() {
      renderer.setRenderTarget(null);
      renderer.render(scene, camera);
    }

    function animate() {
      requestAnimationFrame(animate);
      controls.update();
      render();
      stats.update();
    }

    function initProgressBar(name, length) {
      progressBar = document.getElementById("progressBar");
      progress[name] = new Array(length).fill("🔴");
      progressBar.innerText = Object.keys(progress)
        .map((key) => progress[key].join(""))
        .join("|");
    }
    function updateProgressBar(name, index) {
      progressBar = document.getElementById("progressBar");
      progress[name][index] = "🟢";
      progressBar.innerText = Object.keys(progress)
        .map((key) => progress[key].join(""))
        .join("|");
    }

    function init() {
      console.log("[INFO] initialize...");

      // init webgl
      if (WebGL.isWebGL2Available() === false) {
        document.body.appendChild(WebGL.getWebGL2ErrorMessage());
        return;
      }

      // return error message if conf is empty
      if (Object.keys(scene_names).length === 0) {
        let e = document.createElement("p");
        e.style.cssText = "text-align: center; font-size: 28px;";
        e.innerHTML =
          "<b>Please provide at least one scene as URL parameters:</b> \
        <br> ?scene=lego \
        <br> ?scene=fern \
        <br> ?scene=garden \
        <br> ?scene=lego&scene=chair \
        ";
        document.body.appendChild(e);
        return;
      }

      // create renderer
      container = document.getElementById("container");

      renderer = new THREE.WebGLRenderer({
        powerPreference: "high-performance",
        precision: "mediump",
      });

      renderer.setPixelRatio(1);
      renderer.setSize(configs.W, configs.H);
      renderer.domElement.classList.add("renderer");
      container.appendChild(renderer.domElement);

      stats = new Stats();
      container.appendChild(stats.dom);

      // create camera
      camera = new THREE.PerspectiveCamera(
        configs.fovy,
        configs.W / configs.H,
        configs.near,
        configs.far
      );
      camera.position.y = 2.0;
      camera.position.z = 3.464;
      camera.up.set(0, 0, 1);
      // console.log(camera)
      controls = new OrbitControls(camera, renderer.domElement);
      // controls.enableDamping = true;
      // controls.screenSpacePanning = true;

      // create scene
      scene = new THREE.Scene();
      sceneRef = {};

      // console.log(configs.bg_color);
      scene.background = new THREE.Color(configs.bg_color); // white background

      // window.addEventListener( 'resize', onWindowResize, false );

      // create GUI
      const gui = new GUI();

      gui.addColor(configs, "bg_color").onChange((v) => {
        scene.background = new THREE.Color(v);
      });

      gui.add(configs, "H", 64, Math.max(configs.H, 1024)).onChange((v) => {
        camera.aspect = configs.W / v;
        camera.updateProjectionMatrix();
        renderer.setSize(configs.W, v);
        render();
      });
      gui.add(configs, "W", 64, Math.max(configs.W, 1024)).onChange((v) => {
        camera.aspect = v / configs.H;
        camera.updateProjectionMatrix();
        renderer.setSize(v, configs.H);
        render();
      });
      gui.add(configs, "fovy", 0.001, 180).onChange((v) => {
        camera.fov = v;
        camera.updateProjectionMatrix();
        render();
      });
      gui.add(configs, "near", 0.001, 10).onChange((v) => {
        camera.near = v;
        camera.updateProjectionMatrix();
        render();
      });
      gui.add(configs, "far", 0.001, 1000).onChange((v) => {
        camera.far = v;
        camera.updateProjectionMatrix();
        render();
      });

      // load camera pose
      if (configs["cameraState"] !== null) {
        camera.matrix.fromArray(JSON.parse(configs["cameraState"]));
        camera.matrix.decompose(
          camera.position,
          camera.quaternion,
          camera.scale
        );
        camera.updateProjectionMatrix();
        controls.update();
      }

      // separate config per scene,不同场景不同的设置
      scene_names.forEach((name, index) => {
        configs[name] = {
          renderMode: 2, // rendering mode: 0 = normal, 1 = diffuse, 2 = specular.
          pos_x: parseFloat(params.get(name + ".pos_x")) || 0,
          pos_y: parseFloat(params.get(name + ".pos_y")) || 0,
          pos_z: parseFloat(params.get(name + ".pos_z")) || 0,
          scale_x: parseFloat(params.get(name + ".scale_x")) || 1,
          scale_y: parseFloat(params.get(name + ".scale_y")) || 1,
          scale_z: parseFloat(params.get(name + ".scale_z")) || 1,
          rot_x: parseFloat(params.get(name + ".rot_x")) || 0,
          rot_y: parseFloat(params.get(name + ".rot_y")) || 0,
          rot_z: parseFloat(params.get(name + ".rot_z")) || 0,
        };
        const folder = gui.addFolder(name);
        folder
          .add(configs[name], "renderMode", {
            normal: 0,
            diffuse: 1,
            specular: 2,
          })
          .onChange((v) => {
            sceneRef[name].forEach((object, index) => {
              object.traverse(function (child) {
                if (child.type == "Mesh") {
                  child.material.uniforms["mode"]["value"] = v;
                }
              });
            });
          });
        folder.add(configs[name], "pos_x", -10, 10).onChange((v) => {
          sceneRef[name].forEach((object, index) => {
            object.position.x = v;
          });
        });
        folder.add(configs[name], "pos_y", -10, 10).onChange((v) => {
          sceneRef[name].forEach((object, index) => {
            object.position.y = v;
          });
        });
        folder.add(configs[name], "pos_z", -10, 10).onChange((v) => {
          sceneRef[name].forEach((object, index) => {
            object.position.z = v;
          });
        });
        folder.add(configs[name], "scale_x", 0, 5).onChange((v) => {
          sceneRef[name].forEach((object, index) => {
            object.scale.x = v;
          });
        });
        folder.add(configs[name], "scale_y", 0, 5).onChange((v) => {
          sceneRef[name].forEach((object, index) => {
            object.scale.y = v;
          });
        });
        folder.add(configs[name], "scale_z", 0, 5).onChange((v) => {
          sceneRef[name].forEach((object, index) => {
            object.scale.z = v;
          });
        });
        folder.add(configs[name], "rot_x", 0, 360).onChange((v) => {
          sceneRef[name].forEach((object, index) => {
            object.rotation.x = (v / 180) * Math.PI;
          });
        });
        folder.add(configs[name], "rot_y", 0, 360).onChange((v) => {
          sceneRef[name].forEach((object, index) => {
            object.rotation.y = (v / 180) * Math.PI;
          });
        });
        folder.add(configs[name], "rot_z", 0, 360).onChange((v) => {
          sceneRef[name].forEach((object, index) => {
            object.rotation.z = (v / 180) * Math.PI;
          });
        });
        folder.close(); // collapsed by default
      });

      configs["save config URL"] = () => {
        // construct a URL string that repeat current configs
        let base = window.location.href.split("?")[0];
        function unwrap(x, prefix = "") {
          let res = [];
          for (const key of Object.keys(x)) {
            // leave out default values
            if (
              (key.includes("pos") && x[key] === 0) ||
              (key.includes("scale") && x[key] === 1) ||
              (key.includes("rot") && x[key] === 0) ||
              (key === "renderMode" && x[key] === 0)
            )
              continue;
            res.push(prefix + key + "=" + String(x[key]));
          }
          return res.join("&");
        }
        let res = [];
        for (const key of Object.keys(configs)) {
          if (
            key == "save config URL" ||
            (key === "fovy" && configs[key] === 60) ||
            (key === "near" && configs[key] === 0.01) ||
            (key === "far" && configs[key] === 100) ||
            (key === "bg_color" && configs[key] === 0xffffff)
          ) {
            continue;
          } else if (key == "cameraState") {
            res.push("cameraState=" + JSON.stringify(camera.matrix.toArray()));
          } else if (configs[key].constructor == Object) {
            res.push("scene=" + key);
            res.push(unwrap(configs[key], key + "."));
          } else {
            res.push(key + "=" + String(configs[key]));
          }
        }
        prompt("Copy to clipboard: Ctrl+C, Enter", base + "?" + res.join("&"));
      };

      gui.add(configs, "save config URL");

      // load all scenes async
      let promises = [];
      progress = {};

      // scene_names.forEach((name,index)=>console.log((name,index)));
      console.log("the name of the scene is: ", scene_names);

      // scene_names.forEach(name=>console.log(name));
      // scene_names = Array.from(scene_names);
      // scene_names.forEach((name,index)=>console.log((name)));
      // console.log("[INFO] test:");

      scene_names.forEach((name, index) => {
        promises.push(
          fetch(BASE + name + "/mlp.json")
            .then((response) => {
              return response.json();
            })
            .catch((error) => {
              console.error("Error:", error);
            })
            .then((network_weights) => {
              console.log("[INFO] loading:", name);

              // check bound, load all meshes
              // let bound = network_weights["bound"];
              // let cascade = network_weights["cascade"];
              let cascade = 5; //存储到8张图片（每张图片4维）

              initProgressBar(name, cascade);
              sceneRef[name] = [];

              // for (let cas = 0; cas < cascade; cas++) {//多张图片的存储
              // load feature texture,漫反射材质与镜面反射材质
              let tex0 = new THREE.TextureLoader().load(
                BASE + name + "/feat012_" + ".png",
                (object) => {
                  console.log("[INFO] loaded feature0:", name);
                  updateProgressBar(name, 0);
                }
              );
              console.log("the tex0 is: ", tex0);
              let tex1 = new THREE.TextureLoader().load(
                BASE + name + "/feat234_" + ".png",
                (object) => {
                  console.log("[INFO] loaded feature1:", name);
                  updateProgressBar(name, 1);
                }
              );

              let diffuse = new THREE.TextureLoader().load(
                BASE + name + "/diffse_float32" + ".png",
                (object) => {
                  console.log("[INFO] loaded diffuse", name);
                  updateProgressBar(name, 5);
                }
              );

              // console.log("the tex4 is: ", tex4);
              tex0.magFilter = THREE.NearestFilter;
              tex0.minFilter = THREE.NearestFilter;
              tex1.magFilter = THREE.NearestFilter;
              tex1.minFilter = THREE.NearestFilter;
              diffuse.magFilter = THREE.NearestFilter;
              diffuse.minFilter = THREE.NearestFilter;

              // tex0.magFilter = THREE.MipMapLinearFilter;
              // tex0.minFilter = THREE.MipMapLinearFilter;
              // tex1.magFilter = THREE.MipMapLinearFilter;
              // tex1.minFilter = THREE.MipMapLinearFilter;
              // diffuse.magFilter = THREE.MipMapLinearFilter;
              // diffuse.minFilter = THREE.MipMapLinearFilter;
              
              // tex0.magFilter = THREE.LinearFilter;
              // tex0.minFilter = THREE.LinearFilter;
              // tex1.magFilter = THREE.LinearFilter;
              // tex1.minFilter =THREE.LinearFilter;
              // diffuse.magFilter = THREE.LinearFilter;
              // diffuse.minFilter = THREE.LinearFilter;

              // load MLP
              let RenderFragShader = //将参数传入片段着色器
                createViewDependenceFunctions(network_weights);
              // console.log("片段着色器是: ", RenderFragShader);
              let weightsTexZero = createNetworkWeightTexture(
                network_weights["net.0.weight"]
              );
              console.log("weightsTexZero is: ", weightsTexZero);
              let weightsTexOne = createNetworkWeightTexture(
                network_weights["net.1.weight"]
              );
              console.log("weightsTexOne is: ", weightsTexOne);

              let weightsBiasZero = createNetworkBias(
                network_weights["net.0.bias"]
              );
              console.log("weightsBiasZero is: ", weightsBiasZero);

              let weightsBiasOne = createNetworkBias(
                network_weights["net.1.bias"]
              );
              console.log("weightsBiasOne is: ", weightsBiasOne);

              let newmat = new THREE.RawShaderMaterial({
                side: THREE.DoubleSide,
                vertexShader: RenderVertShader,
                fragmentShader: RenderFragShader,
                uniforms: {
                  mode: { value: configs[name].renderMode },
                  // tDiffuse: { value: tex0 },
                  // tSpecular: { value: tex1 },
                  // hash:{value:tex},//加载的hash特征
                  hash0: { value: tex0 },
                  hash1: { value: tex1 },
                  Diffuse: { value: diffuse },
                  weightsZero: { value: weightsTexZero },
                  weightsOne: { value: weightsTexOne },
                  BiasZero: { value: weightsBiasZero },
                  BiasOne: { value: weightsBiasOne },
                },
                glslVersion: THREE.GLSL3,
              });
              console.log("渲染的颜色材质 ", newmat);

              // load obj
              let obj_mesh = new OBJLoader().load(
                // BASE + name + "/mesh_" + cas.toString() + ".obj",
                BASE + name + "/mesh_0" + ".obj",
                (object) => {
                  object.traverse(function (child) {
                    if (child.type == "Mesh") {
                      child.material = newmat;
                    }
                  });
                  console.log("[INFO] loaded mesh:", name);
                  updateProgressBar(name, 8 * 3);
                  object.position.set(
                    configs[name].pos_x,
                    configs[name].pos_y,
                    configs[name].pos_z
                  );
                  object.scale.set(
                    configs[name].scale_x,
                    configs[name].scale_y,
                    configs[name].scale_z
                  );
                  object.rotation.set(
                    (configs[name].rot_x / 180) * Math.PI,
                    (configs[name].rot_y / 180) * Math.PI,
                    (configs[name].rot_z / 180) * Math.PI
                  );
                  sceneRef[name].push(object);
                  scene.add(object);
                }
              );
              console.log("加载的obj文件是: ", obj_mesh);
            })
            .catch((error) => {
              console.error("Error:", error);
            })
        );
      });

      Promise.all(promises).then((response) => {
        console.log("[INFO] start animation!");
        animate();
      });
    }

    init();
  </script>
</html>